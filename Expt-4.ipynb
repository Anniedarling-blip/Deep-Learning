{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "sentence = \"deeplearning is amazing, deeplearning builds intelligent\"\n",
        "\n",
        "\n",
        "words = sentence.split()\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([sentence])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for i in range(1, len(words)):\n",
        "    n_gram_sequence = words[:i+1]\n",
        "    input_sequences.append(tokenizer.texts_to_sequences([' '.join(n_gram_sequence)])[0])\n",
        "\n",
        "\n",
        "max_seq_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "xs, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 50, input_length=max_seq_len-1),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(xs, ys, epochs=500, verbose=0)\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, max_seq_len):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = model.predict(sequence, verbose=0)\n",
        "    return tokenizer.index_word[np.argmax(predicted)]\n",
        "\n",
        "for i in range(len(words)):\n",
        "    current_text = ' '.join(words[:i+1])\n",
        "    next_word = predict_next_word(model, tokenizer, current_text, max_seq_len)\n",
        "    print(f\"Input: '{current_text}' -> Predicted Next Word: '{next_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9DwxDOZs2_3",
        "outputId": "92d44027-009e-4dee-be67-e802d689c4dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'deeplearning' -> Predicted Next Word: 'is'\n",
            "Input: 'deeplearning is' -> Predicted Next Word: 'amazing'\n",
            "Input: 'deeplearning is amazing,' -> Predicted Next Word: 'deeplearning'\n",
            "Input: 'deeplearning is amazing, deeplearning' -> Predicted Next Word: 'builds'\n",
            "Input: 'deeplearning is amazing, deeplearning builds' -> Predicted Next Word: 'intelligent'\n",
            "Input: 'deeplearning is amazing, deeplearning builds intelligent' -> Predicted Next Word: 'intelligent'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = [\n",
        "    (\"Deep learning is\", \"amazing\"),\n",
        "    (\"Deep learning builds intelligent\", \"systems\"),\n",
        "    (\"Intelligent systems can learn\", \"quickly\")\n",
        "]\n",
        "\n",
        "\n",
        "all_text = \" \".join([item[0] + \" \" + item[1] for item in data])\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([all_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for input_text, expected_word in data:\n",
        "    text = input_text + \" \" + expected_word\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_seq_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "xs, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 50, input_length=max_seq_len-1),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(xs, ys, epochs=500, verbose=0)\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, max_seq_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    return tokenizer.index_word[np.argmax(predicted)]\n",
        "\n",
        "# Generate predictions and check correctness\n",
        "results = []\n",
        "for input_text, expected_word in data:\n",
        "    predicted_word = predict_next_word(model, tokenizer, input_text, max_seq_len)\n",
        "    correct = \"Y\" if predicted_word.lower() == expected_word.lower() else \"N\"\n",
        "    results.append([input_text, predicted_word, correct])\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"Input Text\", \"Predicted Word\", \"Correct (Y/N)\"])\n",
        "print(df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpIomuxPtBRl",
        "outputId": "b82a860f-55dc-4a5b-b6c2-cda30ff7115c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Input Text Predicted Word Correct (Y/N)\n",
            "                Deep learning is        amazing             Y\n",
            "Deep learning builds intelligent        systems             Y\n",
            "   Intelligent systems can learn        quickly             Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "corpus = [\n",
        "    \"Shall I compare thee to a summer's day\",\n",
        "    \"Thou art more lovely and more temperate\",\n",
        "    \"Rough winds do shake the darling buds of May\",\n",
        "    \"And summer's lease hath all too short a date\"\n",
        "]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_len-1),\n",
        "    LSTM(150),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X, y, epochs=50, verbose=1, validation_split=0.2)\n",
        "\n",
        "def predict_next_words(model, tokenizer, text, next_words=3):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        output_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "        text += \" \" + output_word\n",
        "    return text\n",
        "\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(predict_next_words(model, tokenizer, \"Shall I compare\", 5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8mreOVZtB7N",
        "outputId": "1b84be71-f2ab-423f-e1c7-d8cef8a386f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 3.4004 - val_accuracy: 0.0000e+00 - val_loss: 3.4095\n",
            "Epoch 2/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.1304 - loss: 3.3903 - val_accuracy: 0.0000e+00 - val_loss: 3.4150\n",
            "Epoch 3/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.1739 - loss: 3.3801 - val_accuracy: 0.0000e+00 - val_loss: 3.4211\n",
            "Epoch 4/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.3043 - loss: 3.3692 - val_accuracy: 0.0000e+00 - val_loss: 3.4285\n",
            "Epoch 5/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.3043 - loss: 3.3574 - val_accuracy: 0.0000e+00 - val_loss: 3.4374\n",
            "Epoch 6/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.3478 - loss: 3.3441 - val_accuracy: 0.0000e+00 - val_loss: 3.4486\n",
            "Epoch 7/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.3913 - loss: 3.3288 - val_accuracy: 0.0000e+00 - val_loss: 3.4632\n",
            "Epoch 8/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.3043 - loss: 3.3107 - val_accuracy: 0.0000e+00 - val_loss: 3.4825\n",
            "Epoch 9/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.3043 - loss: 3.2887 - val_accuracy: 0.0000e+00 - val_loss: 3.5089\n",
            "Epoch 10/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.2609 - loss: 3.2616 - val_accuracy: 0.0000e+00 - val_loss: 3.5459\n",
            "Epoch 11/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.2609 - loss: 3.2278 - val_accuracy: 0.0000e+00 - val_loss: 3.5993\n",
            "Epoch 12/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.2609 - loss: 3.1857 - val_accuracy: 0.0000e+00 - val_loss: 3.6788\n",
            "Epoch 13/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.2609 - loss: 3.1344 - val_accuracy: 0.0000e+00 - val_loss: 3.8006\n",
            "Epoch 14/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.1739 - loss: 3.0756 - val_accuracy: 0.0000e+00 - val_loss: 3.9907\n",
            "Epoch 15/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.1739 - loss: 3.0177 - val_accuracy: 0.0000e+00 - val_loss: 4.2754\n",
            "Epoch 16/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.1304 - loss: 2.9775 - val_accuracy: 0.0000e+00 - val_loss: 4.6225\n",
            "Epoch 17/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.1304 - loss: 2.9625 - val_accuracy: 0.0000e+00 - val_loss: 4.9152\n",
            "Epoch 18/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.1304 - loss: 2.9478 - val_accuracy: 0.0000e+00 - val_loss: 5.0955\n",
            "Epoch 19/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.1739 - loss: 2.9153 - val_accuracy: 0.0000e+00 - val_loss: 5.1782\n",
            "Epoch 20/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.1739 - loss: 2.8704 - val_accuracy: 0.0000e+00 - val_loss: 5.1966\n",
            "Epoch 21/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.1739 - loss: 2.8249 - val_accuracy: 0.0000e+00 - val_loss: 5.1830\n",
            "Epoch 22/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.1739 - loss: 2.7851 - val_accuracy: 0.0000e+00 - val_loss: 5.1651\n",
            "Epoch 23/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.1739 - loss: 2.7506 - val_accuracy: 0.0000e+00 - val_loss: 5.1659\n",
            "Epoch 24/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.2174 - loss: 2.7178 - val_accuracy: 0.0000e+00 - val_loss: 5.2030\n",
            "Epoch 25/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.2609 - loss: 2.6821 - val_accuracy: 0.0000e+00 - val_loss: 5.2895\n",
            "Epoch 26/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.3043 - loss: 2.6392 - val_accuracy: 0.0000e+00 - val_loss: 5.4320\n",
            "Epoch 27/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.2609 - loss: 2.5867 - val_accuracy: 0.0000e+00 - val_loss: 5.6291\n",
            "Epoch 28/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.3043 - loss: 2.5240 - val_accuracy: 0.0000e+00 - val_loss: 5.8681\n",
            "Epoch 29/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.2609 - loss: 2.4536 - val_accuracy: 0.0000e+00 - val_loss: 6.1224\n",
            "Epoch 30/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.3913 - loss: 2.3816 - val_accuracy: 0.0000e+00 - val_loss: 6.3503\n",
            "Epoch 31/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.4783 - loss: 2.3112 - val_accuracy: 0.0000e+00 - val_loss: 6.5102\n",
            "Epoch 32/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.4348 - loss: 2.2346 - val_accuracy: 0.0000e+00 - val_loss: 6.5971\n",
            "Epoch 33/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5217 - loss: 2.1444 - val_accuracy: 0.0000e+00 - val_loss: 6.6366\n",
            "Epoch 34/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.4783 - loss: 2.0480 - val_accuracy: 0.0000e+00 - val_loss: 6.6623\n",
            "Epoch 35/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.4348 - loss: 1.9569 - val_accuracy: 0.0000e+00 - val_loss: 6.6999\n",
            "Epoch 36/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.6087 - loss: 1.8650 - val_accuracy: 0.0000e+00 - val_loss: 6.7814\n",
            "Epoch 37/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.6957 - loss: 1.7813 - val_accuracy: 0.0000e+00 - val_loss: 6.8996\n",
            "Epoch 38/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6957 - loss: 1.7014 - val_accuracy: 0.0000e+00 - val_loss: 6.9967\n",
            "Epoch 39/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7391 - loss: 1.6250 - val_accuracy: 0.0000e+00 - val_loss: 7.0628\n",
            "Epoch 40/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.6957 - loss: 1.5519 - val_accuracy: 0.0000e+00 - val_loss: 7.1317\n",
            "Epoch 41/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.7391 - loss: 1.4856 - val_accuracy: 0.0000e+00 - val_loss: 7.2045\n",
            "Epoch 42/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7391 - loss: 1.4247 - val_accuracy: 0.0000e+00 - val_loss: 7.3071\n",
            "Epoch 43/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7826 - loss: 1.3665 - val_accuracy: 0.0000e+00 - val_loss: 7.4420\n",
            "Epoch 44/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7826 - loss: 1.3125 - val_accuracy: 0.0000e+00 - val_loss: 7.4979\n",
            "Epoch 45/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8261 - loss: 1.2578 - val_accuracy: 0.0000e+00 - val_loss: 7.5246\n",
            "Epoch 46/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7391 - loss: 1.2105 - val_accuracy: 0.0000e+00 - val_loss: 7.5993\n",
            "Epoch 47/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8261 - loss: 1.1646 - val_accuracy: 0.0000e+00 - val_loss: 7.6889\n",
            "Epoch 48/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8261 - loss: 1.1242 - val_accuracy: 0.0000e+00 - val_loss: 7.8602\n",
            "Epoch 49/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8696 - loss: 1.0935 - val_accuracy: 0.0000e+00 - val_loss: 7.7888\n",
            "Epoch 50/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.7826 - loss: 1.0640 - val_accuracy: 0.0000e+00 - val_loss: 7.9239\n",
            "Generated text:\n",
            "Shall I compare thee to a summer's day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    \"To be or not to be\",\n",
        "    \"What light through yonder window breaks\",\n",
        "    \"O Romeo Romeo wherefore art thou Romeo\",\n",
        "    \"Parting is such sweet sorrow\",\n",
        "    \"Shall I compare thee to a summer's day\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_len-1),\n",
        "    LSTM(150),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X, y, epochs=50, verbose=0)\n",
        "\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, max_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    return tokenizer.index_word[np.argmax(predicted)]\n",
        "\n",
        "test_data = [\n",
        "    (\"To be or not\", \"to\"),\n",
        "    (\"What light through yonder\", \"window\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "for input_seq, expected_word in test_data:\n",
        "    predicted_word = predict_next_word(model, tokenizer, input_seq, max_len)\n",
        "    correct = \"Y\" if predicted_word.lower() == expected_word.lower() else \"N\"\n",
        "    results.append([input_seq, predicted_word, correct])\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"Input Sequence\", \"Predicted Word\", \"Correct (Y/N)\"])\n",
        "print(df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LzDkWGGttPY",
        "outputId": "d78d2cc9-9b2b-493c-ad99-349a538fdc5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Input Sequence Predicted Word Correct (Y/N)\n",
            "             To be or not             to             Y\n",
            "What light through yonder         window             Y\n"
          ]
        }
      ]
    }
  ]
}